{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5VNTxW34d+j8M7lVD7/76",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shashanksrajak/neural-networks-from-zero/blob/main/1_neural_net_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing Neural Network from Scratch\n",
        "The goal is to understand neural network and its inner working and then build one from scratch using all the mathematical concepts and tools."
      ],
      "metadata": {
        "id": "klh270XrF4Qz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import math\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "pBcnO7czHMK1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Single Neuron\n",
        "Here I used `Numpy` to vectorize the calculation but this can also be done in raw python using `zip` and then multiplying element wise.\n",
        "\n",
        "![Neuron](https://cs231n.github.io/assets/nn1/neuron_model.jpeg)"
      ],
      "metadata": {
        "id": "u7LhQKB4GJZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "  return 1/(1+math.exp(-z))"
      ],
      "metadata": {
        "id": "WLLSnbU9R4kI"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Neuron:\n",
        "  def __init__(self, nin):\n",
        "    \"\"\"\n",
        "    A single neuron\n",
        "    nin : number of inputs to this neuron\n",
        "    \"\"\"\n",
        "    self.w = np.array([random.uniform(-1, 1) for _ in range(nin)]) # weights\n",
        "    self.b = random.uniform(-1, 1) # bias term\n",
        "\n",
        "  def __call__(self, x):\n",
        "    z = np.dot(self.w, x) + self.b # z = wx + b\n",
        "    a = sigmoid(z)   # we are using sigmoid function but this can also be a user choice\n",
        "\n",
        "    self.out = a\n",
        "    return a # scalar value; one neuron outputs one scalar"
      ],
      "metadata": {
        "id": "bBirH97OGIlB"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jksRZKe6Fv7K"
      },
      "outputs": [],
      "source": [
        "n = Neuron(3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sample input\n",
        "x = np.array([random.randint(1, 10) for _ in range(3)])\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1HmgsQYKfVC",
        "outputId": "9b61bb92-f97e-4720-8e61-2d40aeea4da4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 5, 10,  1])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jJ4hLflKxgM",
        "outputId": "d321da65-2599-42bb-8d1a-e664e91e2e64"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9878307196697438"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n.w"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRUOXBEoLC9N",
        "outputId": "701c6f9b-193c-4b66-db14-e280f48a4f3c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.77995667,  0.94358218, -0.87519861])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n.b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPNCHS0uLHha",
        "outputId": "a1f7d2dc-80fa-4bd2-c580-1aed67dce326"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.26424324169995606"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bO20PFMdLItp",
        "outputId": "84406193-3e31-4d79-c8b6-68eac85d97db"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9878307196697438"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Layer of Neurons\n",
        "\n",
        "Now we have a single neuron, we can stack multiple neurons together to build a `Layer` of neurons with each neuron doing some calculations as per its own `w` and `b` parameters.\n",
        "\n",
        "![layer](https://miro.medium.com/v2/resize:fit:1200/1*zdVSuUJMnW_HQiVcRzevTQ.png)\n"
      ],
      "metadata": {
        "id": "up4spbCPOE4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer:\n",
        "  def __init__(self, nin, n) -> None:\n",
        "    \"\"\"\n",
        "    nin : number of inputs to this layer\n",
        "    n : number of neurons in this layer\n",
        "    \"\"\"\n",
        "    self.neurons = [Neuron(nin) for _ in range(n)]\n",
        "\n",
        "  def __call__(self, x):\n",
        "    outs = [neuron(x) for neuron in self.neurons]\n",
        "    return outs"
      ],
      "metadata": {
        "id": "H259egxlLJmk"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l = Layer(3, 2)"
      ],
      "metadata": {
        "id": "GIoGqC8GQK_Q"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l.neurons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrEzPfjhQWKv",
        "outputId": "87520eb9-7af0-45cd-a8de-43d96d85ccd8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<__main__.Neuron at 0x793f5b92d610>, <__main__.Neuron at 0x793f5b92cb90>]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlIOGCDZQXIK",
        "outputId": "58bbdfd8-5e98-4102-d881-d0df41f9dfc6"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 5 10  1]\n",
            "[-0.07167917 -6.95480187 -0.1451056 ]\n",
            "-7.001496776705659\n",
            "[ 5 10  1]\n",
            "[ 1.138274    2.39467073 -0.88312817]\n",
            "3.3962792585434465\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5002274224377617, 0.7246384841220469]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch Training\n",
        "Now lets say we want to train a batch of examples, then one way is to use a for loop and iterate for each training example. Another way is vectorization which we will see afterwards."
      ],
      "metadata": {
        "id": "ZH0Q6gb9afvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([[2, 4, 5], [3, 5, 7], [1, 3, 6], [4, 8, 10]]) # 4 training examples\n",
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CS1B6Vw3asL3",
        "outputId": "57c120df-89b5-45dd-864f-c812daf204af"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 2,  4,  5],\n",
              "       [ 3,  5,  7],\n",
              "       [ 1,  3,  6],\n",
              "       [ 4,  8, 10]])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets create a `Layer` of 2 neurons so we get 2 outputs per example."
      ],
      "metadata": {
        "id": "SqvJhKAsa9BA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer = Layer(3, 2)"
      ],
      "metadata": {
        "id": "nGwpEsuba0NG"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outs = [layer(example) for example in X]\n",
        "outs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPRDDZiJba9o",
        "outputId": "1fe8a0e3-0802-4d52-dd55-7a297296606b"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.22398334825436683, 0.9887969329375543],\n",
              " [0.1342579535452807, 0.9986753763349573],\n",
              " [0.2851572048063245, 0.9940152356750908],\n",
              " [0.14833353781105948, 0.9999216820387337]]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far we have been iterating through loops but its time to use Matrix to do parallel computing and efficient neural net.\n",
        "1. Represent inputs as a matrix: Instead of processing each training example individually, stack them into a single input matrix X, where each row represents a training example.\n",
        "\n",
        "2. Represent weights as a matrix: Similarly, you can arrange the weights of all neurons in a layer into a single weight matrix W, where each column corresponds to a neuron's weights.\n",
        "\n",
        "3. Represent biases as a vector: The biases for all neurons in a layer can be stored in a bias vector b.\n",
        "\n",
        "4. Matrix multiplication for wx: The wx part of the calculation (wx + b) for all neurons and all training examples can be performed efficiently using matrix multiplication. The result will be a matrix where each element represents the wx value for a specific training example and a specific neuron.\n",
        "\n",
        "5. Broadcasting for + b: The bias vector b can be added to the result of the matrix multiplication using broadcasting, which will add the bias to the corresponding wx value for each neuron across all training examples."
      ],
      "metadata": {
        "id": "nX2iQX-nh8N5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will implement the `Layer` class again to use vectorization and let the `Neuron` class be as it is.\n",
        "\n",
        "We also redefined the `sigmoid` function to use `np.exp` just to vectorize the operations"
      ],
      "metadata": {
        "id": "D69I2iOFiUFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "  return 1/(1+np.exp(-z))\n",
        "\n",
        "class Layer:\n",
        "  def __init__(self, nin, n) -> None:\n",
        "    \"\"\"\n",
        "    nin : number of inputs to this layer\n",
        "    n : number of neurons in this layer\n",
        "    \"\"\"\n",
        "    self.neurons = [Neuron(nin) for _ in range(n)]\n",
        "    neuron_ws = np.array([neuron.w for neuron in self.neurons])\n",
        "\n",
        "    self.W = neuron_ws.T\n",
        "    self.b = np.array([neuron.b for neuron in self.neurons])\n",
        "\n",
        "  def __call__(self, X):\n",
        "    \"\"\"\n",
        "    X : input matrix\n",
        "    \"\"\"\n",
        "    # outs = [neuron(x) for neuron in self.neurons]\n",
        "    # instead of this iteration and using call method of neuron we will use mat mul here\n",
        "    Z = np.matmul(X, self.W) + self.b\n",
        "\n",
        "    # outs = [[sigmoid(z) for z in row] for row in Z]\n",
        "    # instead of this list comprehension we can directly call sigmoid because that is vectorized now\n",
        "    outs = sigmoid(Z)\n",
        "\n",
        "    return outs"
      ],
      "metadata": {
        "id": "jsRMMEMaiDyV"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer = Layer(3, 2)"
      ],
      "metadata": {
        "id": "wCvdfzhTjCyn"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer.W # col1 is w for neuron 1 and col2 is w for neuron 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbcW7NXBjF-n",
        "outputId": "050cca10-3792-4cf6-cd24-3bd35f185e61"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.54303226,  0.47411194],\n",
              "       [-0.95306267, -0.35183705],\n",
              "       [ 0.43838726, -0.32361105]])"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mejULACMmMrW",
        "outputId": "c48f7ddb-1a3a-475b-a6cb-c1c0b2585d00"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.03401578, 0.21426184],\n",
              "       [0.0186032 , 0.13891009],\n",
              "       [0.19594388, 0.14863963],\n",
              "       [0.00234598, 0.03303469]])"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UU6_bG8vpqxR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}