{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e00e3633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c62ed8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "  def __init__(self):\n",
    "    self.w = None\n",
    "    self.b = None\n",
    "    self.loss = None\n",
    "\n",
    "  def fit(self, x, y, epoch=10, alpha=0.001):\n",
    "    \"\"\"\n",
    "      x : feature matrix where each row represents one training instance, and col = feature\n",
    "      y : target vector of shape (1, 1) \n",
    "    \"\"\"\n",
    "    # initialize w and b based on the shape of x\n",
    "    n_cols_x = x.shape[1]  # number of cols in x = num of features in x\n",
    "    self.w = np.random.randn(n_cols_x, 1)\n",
    "    self.b = 0\n",
    "    \n",
    "    print(f\"Initial parameters w : {self.w} b: {self.b}\")\n",
    "    \n",
    "    # call gradient descent\n",
    "    self.gradient_descent(x, y, epoch, alpha)\n",
    "\n",
    "  def coefficients_(self):\n",
    "    print(\"w: \", self.w)\n",
    "    print(\"b: \", self.b)\n",
    "\n",
    "  def gradient_descent(self, x, y, epoch, alpha=0.001):\n",
    "    n = len(x) # length of training dataset\n",
    "    \n",
    "    for i in range(epoch):\n",
    "      # first calculate y_predicted for all the samples in x_train\n",
    "      y_predicted = self.predict(x)\n",
    "\n",
    "      # calculate loss\n",
    "      j = self.loss_function(y_predicted, y)   # predicted - actual\n",
    "      self.loss = j\n",
    "\n",
    "      # gradients calculation\n",
    "      dj_dw = 1/n * (x.T @ (y_predicted-y))  # for entire dataset in x\n",
    "      dj_db = np.mean(y_predicted-y)\n",
    "\n",
    "      # paramters update\n",
    "      self.w = self.w - (alpha * dj_dw)\n",
    "      self.b = self.b - (alpha * dj_db)\n",
    "\n",
    "      if i % 100 == 0:\n",
    "        print(f\"Epoch: {i+1}, Loss: {j}, w: {self.w}, b: {self.b}\")\n",
    "\n",
    "  def loss_function(self, y_predicted, y_true):\n",
    "    \"\"\"\n",
    "    Calculating Mean Squared Error\n",
    "    \"\"\"\n",
    "    loss = np.mean((y_predicted - y_true)**2) * 0.5\n",
    "    return loss\n",
    "\n",
    "  def predict(self, x):\n",
    "    z = x @ self.w + self.b\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24761c7c",
   "metadata": {},
   "source": [
    "Test this with a simple function. We have explictly choosen a linear function and we know the weights. So lets see if gradient descent can find similar weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b36c34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "x_train = 2 * np.random.rand(100, 1)\n",
    "y_train = 4 + 3 * x_train + np.random.randn(100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8d32da67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0ede634f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial parameters w : [[2.72016917]] b: 0\n",
      "Epoch: 1, Loss: 9.48631468120293, w: [[3.12270739]], b: 0.4262061778378716\n",
      "Epoch: 101, Loss: 0.4303375120462318, w: [[3.13743293]], b: 3.799080129310186\n",
      "Epoch: 201, Loss: 0.4041740202935006, w: [[2.83643706]], b: 4.139979789970479\n",
      "Epoch: 301, Loss: 0.40332102873278614, w: [[2.78208887]], b: 4.201533053491397\n",
      "Epoch: 401, Loss: 0.40329321919537675, w: [[2.77227569]], b: 4.212647186735833\n",
      "Epoch: 501, Loss: 0.4032923125388442, w: [[2.77050382]], b: 4.21465396835405\n",
      "Epoch: 601, Loss: 0.40329228297970204, w: [[2.77018388]], b: 4.2150163153202636\n",
      "Epoch: 701, Loss: 0.4032922820160042, w: [[2.77012612]], b: 4.215081741135774\n",
      "Epoch: 801, Loss: 0.40329228198458544, w: [[2.77011568]], b: 4.215093554501787\n",
      "Epoch: 901, Loss: 0.4032922819835612, w: [[2.7701138]], b: 4.215095687537737\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epoch=1000, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e36eb5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:  [[2.77011346]]\n",
      "b:  4.215096071216208\n"
     ]
    }
   ],
   "source": [
    "model.coefficients_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f82dc32",
   "metadata": {},
   "source": [
    "So we are able to get w and b close to actual values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
